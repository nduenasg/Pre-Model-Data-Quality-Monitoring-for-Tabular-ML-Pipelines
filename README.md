# Pre-Model-Data-Quality-Monitoring-for-Tabular-ML-Pipelines
A lightweight system to detect schema violations, missing-value anomalies, outliers, and distribution drift before model training, using a reference dataset and statistical checks.
In production ML pipelines, model failures are often caused by upstream data issues rather than algorithmic choices. This project demonstrates how to identify data quality risks prior to training, using reproducible and interpretable checks.
